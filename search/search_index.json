{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome Welcome to to documentation for the data science game API. If you want to make use of the API, PLease reser to the user documentation If you want to make changes to the project, please refer to the developer documentation About the project This is the documentation for an API made for a data science game. It's purpose is to accept requests containing a pipeline of operations related to data. Some examples of these operations are loading data from a dataset, selecting data based on certain features and training a model on data. Requested results are then returned to the user.","title":"introduction"},{"location":"#welcome","text":"Welcome to to documentation for the data science game API. If you want to make use of the API, PLease reser to the user documentation If you want to make changes to the project, please refer to the developer documentation","title":"Welcome"},{"location":"#about-the-project","text":"This is the documentation for an API made for a data science game. It's purpose is to accept requests containing a pipeline of operations related to data. Some examples of these operations are loading data from a dataset, selecting data based on certain features and training a model on data. Requested results are then returned to the user.","title":"About the project"},{"location":"running_docs/","text":"How to run the documentation yourself? Open command prompt Move to Docs/mkdocs/api-docs mkdocs serve If this does not work: pip install mkdocs mkdocs serve If it still does not work you may have to make a virtual environment and run pip install mkdocs in there followed by mkdocs serve","title":"Running docs"},{"location":"running_docs/#how-to-run-the-documentation-yourself","text":"Open command prompt Move to Docs/mkdocs/api-docs mkdocs serve If this does not work: pip install mkdocs mkdocs serve If it still does not work you may have to make a virtual environment and run pip install mkdocs in there followed by mkdocs serve","title":"How to run the documentation yourself?"},{"location":"under-construction/","text":"This part of the docs is still under construction :(","title":"This part of the docs is still under construction"},{"location":"under-construction/#this-part-of-the-docs-is-still-under-construction","text":":(","title":"This part of the docs is still under construction"},{"location":"dev_docs/api/","text":"api.py api.py is the 'head' of the api. This is where all requests come in and are translated from JSON to its internal logic. The api uses Flask . The code From top to bottom APIResult The APIResult dataclass is used as a return value. Functions may return an APIResult where where different return types are possible. @dataclass class APIResult: payload: tuple[int, Operation] | connection | pipelineObject | str = \"\" succes: bool = True HTTPStatusCode:int = 200 payload : The payload is the actual returned value/object. success : Success will be set to False if an exception has occured. In this case the Payload will be a string containing an error message. HTTPStatuscode : Returns what status code the api should respond with. This is 200 by default (OK) and will be changed when an exception occurs. Startup code This code defines the api and sets it up ready to be started. app = Flask(__name__) CORS(app) swagger = Swagger(app) use_cache = False 'CORS(app)' sets up Cross-Origin Resource Sharing 'swagger = Swagger(app)' sets up swagger 'use_cache = False' makes it so be default the cache will not be used. request: status This code defines a request that simply returns \"api online\". This is useful for checking if the api is running and able to receive requests. @app.route('/status', methods=['GET']) def get_status(): return jsonify(\"api online\") request: datasets This code defines a request that returns the files that can be loaded. @app.route('/datasets', methods=['GET']) def get_datasets(): return jsonify(loadOperation().loadable_files) It uses a preset variable in the load_operation.py file. This means it will likely need to be rewritten when the workings of loading data is changed from preset datasets to loading from a database. (This is a planned change.) request: Execute-pipeline This code defines a request to execute a pipeline. @app.route('/execute-pipeline', methods=['POST']) # type: ignore def execute_pipeline(): json = request.get_json(silent=True) request_id = None session_id = None if json is not None: request_id = json.get(\"request_id\") session_id = json.get(\"session_id\") api_res = json_to_pipeline(json) if not api_res.succes: return jsonify_result(api_res.payload, session_id, request_id), api_res.HTTPStatusCode pipobj:pipelineObject = api_res.payload #type: ignore executed = controller.execute_pipeline(pipobj, use_cache, controller.cacheMethod.PIPELINE) if not executed.succes: return jsonify_result(str(f\"There was a problem with a '{executed.operation_of_failure.__class__.__name__}' operation: '\") + str(executed.payload) + \"'\", session_id, request_id), 500 else: json = pipelinememory_to_json(executed.payload) #type: ignore return jsonify_result(json, session_id, request_id), 200 - Getting the JSON and extracting basic info json = request.get_json(silent=True) request_id = None session_id = None if json is not None: request_id = json.get(\"request_id\") session_id = json.get(\"session_id\") - calling json_to_pipeline api_res = json_to_pipeline(json) if not api_res.succes: return jsonify_result(api_res.payload, session_id, request_id), api_res.HTTPStatusCode pipobj:pipelineObject = api_res.payload #type: ignore If json_to_pipeline(json) was not succesful the exception is returned as response. If it was succesful its return value is saved as pipobj. This will be a pipelineObject. - executing pipeline and sending response. executed = controller.execute_pipeline(pipobj, use_cache, controller. cacheMethod.PIPELINE) if not executed.succes: return jsonify_result(str(f\"There was a problem with a '{executed.operation_of_failure.__class__.__name__}' operation: '\") + str(executed.payload) + \"'\", session_id, request_id), 500 else: json = pipelinememory_to_json(executed.payload) #type: ignore return jsonify_result(json, session_id, request_id), 200 json_to_pipeline Translates JSON to internal logic. def json_to_pipeline(json) -> APIResult: if (json is None): return APIResult(\"No json input was received by api.\", False, 400) if \"operations\" not in json: return APIResult(\"Request does not define operations.\", False, 400) if \"connections\" not in json: return APIResult(\"Request does not define connections.\", False, 400) if \"session_id\" not in json: return APIResult(\"Request does not define sessionid.\", False, 400) json_operations = json[\"operations\"] json_connections = json[\"connections\"] pipobj = pipelineObject() pipobj.session_id = json[\"session_id\"] pipobj.request_id = json.get(\"request_id\") api_res = json_to_operations(pipobj, json_operations) if not api_res.succes: return api_res api_res = json_to_connections(pipobj, json_connections) if not api_res.succes: return api_res return APIResult(pipobj) Some simple checks: if (json is None): return APIResult(\"No json input was received by api.\", False, 400) if \"operations\" not in json: return APIResult(\"Request does not define operations.\", False, 400) if \"connections\" not in json: return APIResult(\"Request does not define connections.\", False, 400) if \"session_id\" not in json: return APIResult(\"Request does not define sessionid.\", False, 400) Defining variables: json_operations = json[\"operations\"] json_connections = json[\"connections\"] pipobj = pipelineObject() pipobj.session_id = json[\"session_id\"] pipobj.request_id = json.get(\"request_id\") Translating operations : api_res = json_to_operations(pipobj, json_operations) if not api_res.succes: return api_res Translating connections : api_res = json_to_connections(pipobj, json_connections) if not api_res.succes: return api_res return APIResult(pipobj) json to operations It converts a list of JSON operations into a list of operations and adds these to the pipelineObject. This uses operation_to_result def json_to_operations(pipobj:pipelineObject, json_operations:list) -> APIResult: results: list[APIResult] = [] operation_hashes_by_id = {} operations_by_id = {} pipeline:list[Operation] = [] for item in json_operations: api_res = operation_to_result(item) if (api_res.succes): results.append(api_res) else: return api_res for res in results: if type(res.payload) == tuple: payload = res.payload operations_by_id[payload[0]] = payload[1] operation_hashes_by_id[payload[0]] = payload[1].hash() #type: ignore pipeline.append(payload[1]) else: return APIResult(f\"unworkable payload detected while preparing pipeline. Payload should have been tuple(int, Operation). Payload given: {api_res.payload} \", False, 500) pipobj.operations = pipeline pipobj.operations_by_id = operations_by_id pipobj.operation_hashes_by_id = operation_hashes_by_id return APIResult(pipobj) json to connections It converts a list of JSON connections into a list of connections and adds these to the pipelineObject. This uses connection_to_result def json_to_connections(pipobj:pipelineObject, json_connections:list) -> APIResult: results = [] for con in json_connections: api_res = connection_to_result(con, pipobj.operations_by_id) if (api_res.succes): results.append(api_res) else: return api_res connections:list[connection] = [] for res in results: if type(res.payload) == connection: connections.append(res.payload) else: return APIResult(f\"unworkable connection detected while preparing pipeline. Payload should have been connection. Payload given: {api_res.payload}\", False, 500) pipobj.connections = connections return APIResult(pipobj) operation to result It takes an operation defined in JSON and translates it to an operation object of the correct class. It may also perform some checks. def operation_to_result(json_operation: dict[str, str]) -> APIResult: operation:Operation id:int = 0 #get operation match json_operation: case {\"operation\": \"load\"}: params = json_operation.get(\"parameters\") check = check_parameters(params, \"load\") if check.succes == False: return check operation = loadOperation() insertion_result = operation.insert_parameters(params) #type: ignore if type(insertion_result) == Exception: return APIResult(\"A load operation had a problem while inserting parameters. Gave following Exception: \" + str(insertion_result), False, 400) case {\"operation\": \"duplicate\"}: operation = duplicateOperation() case {\"operation\":\"select_columns\"}: operation = selectColumnsOperation() params = json_operation.get(\"parameters\") check = check_parameters(params, \"select_columns\") if check.succes == False: return check insertion_result = operation.insert_parameters(params) #type: ignore if type(insertion_result) == Exception: return APIResult(\"A select_columns operation had a problem while inserting parameters. Gave following Exception: \" + str(insertion_result), False, 400) case {\"operation\":\"merge\"}: params = json_operation.get(\"parameters\") check = check_parameters(params, \"merge\") if check.succes == False: return check operation = mergeOperation() insertion_result = operation.insert_parameters(params) #type: ignore if type(insertion_result) == Exception: return APIResult(\"A merge operation had a problem while inserting parameters. Gave following Exception: \" + str(insertion_result), False, 400) case {\"operation\": \"decision_tree\"}: params = json_operation.get(\"parameters\") check = check_parameters(params, \"decision tree\") if check.succes == False: return check operation = decisionTree() insertion_result = operation.insert_parameters(params) #type: ignore if type(insertion_result) == Exception: return APIResult(\"A decision tree operation had a problem while inserting parameters. Gave following Exception: \" + str(insertion_result), False, 400) case {\"operation\":\"filter_rows\"}: params = json_operation.get(\"parameters\") operation = filterRowsOperation() insertion_result = operation.insert_parameters(params) # type: ignore if type(insertion_result) == Exception: return APIResult(\"A filter rows operation had a problem while inserting parameters. Gave following Exception: \" + str(insertion_result), False, 400) case {\"operation\":\"test_operation\"}: operation = controller.testOperation() case {\"operation\":\"predict\"}: params = json_operation.get(\"parameters\") operation = predict() insertion_result = operation.insert_parameters(params) if type(insertion_result) == Exception: return APIResult(\"A predict operation had a problem while inserting parameters. Gave following Exception: \" + str(insertion_result), False, 400) case {\"operation\": notImpl}: return APIResult(f\"Operation {notImpl} is not implemented\", False, 400) case _: return APIResult(f\"A requested operation is not implemented. Could not find implemented operation regarding the following: {json_operation}\", False, 404) # test id match json_operation: case {\"id\": _id} if type(_id) == int: id = _id case {\"id\": _id}: return APIResult(f\"The following id was found not to be convertible to an integer. id: {_id}.\", False, 400) case _: return APIResult(\"An operation was found not to have an id.\", False, 400) return APIResult(tuple([id, operation])) #type: ignore connection to result It takes a connection defined in JSON and translates it to a connection object. Connectiondict should be the connection in JSON. operations should be a dictionary of operations with their id as the keys. def connection_to_result(connectiondict: dict[str, int], operations:dict[int, Operation]) -> APIResult: return APIResult(connection(operations[connectiondict[\"from\"]], operations[connectiondict[\"to\"]], {cport.FROM:int(connectiondict[\"from_port\"]), cport.TO:int(connectiondict[\"to_port\"])} )) pipelinememory to JSON This converts a pipelineObject into JSON. This is used to convert internal logic back to JSON so it can be given as response. def pipelinememory_to_json(pipobj:pipelineObject) -> dict[str, str | dict]: pipmem:pipelineMemory = pipobj.pipeline_memory json: dict[str, str | dict] = {} ids_by_operation:dict[Operation, int] = invert_dictionary(pipobj.operations_by_id) for key, value in pipmem.operations_memory.items(): operation:Operation = key[0] port:int = key[1] id:int= ids_by_operation[operation] jsonpipe:dict[str, str] = {} match value: case val if isinstance(val, pd.DataFrame): jsonpipe[\"output\"] = dataframe_to_json(val) jsonpipe[\"output_type\"] = \"data\" case val if isinstance(val, int): jsonpipe[\"output\"] = str(value) jsonpipe[\"output_type\"] = \"integer\" case val if isinstance(val, list): jsonpipe[\"output\"] = str(value) jsonpipe[\"output_type\"] = \"array\" pipe_output = json.get(\"id_\" + str(id)) if type(pipe_output) == dict: pipe_output.update({\"port_\" + str(port): jsonpipe}) else: pipe_output = {\"port_\" + str(port): jsonpipe} json[\"id_\" + str(id)] = pipe_output return json def operation_to_json(operation:Operation): return str(type(operation)) def invert_dictionary(dict:dict) -> dict: return {v: k for k, v in dict.items()} The start function This function starts the api. if __name__ == '__main__': parser = argparse.ArgumentParser(description='Run the Flask application.') parser.add_argument('--host', type=str, default=\"127.0.0.1\", help='The host') parser.add_argument('--port', type=int, default=5000, help='Port number to run the server on') parser.add_argument('--cache', default=False, action='store_true', help='If true, uses redis cache for better performance') parser.add_argument('--debug', default=False, action='store_true', help='Start in debug mode') args = parser.parse_args() use_cache = args.cache app.run(host=args.host,port=args.port, debug=args.debug) Startup flags When starting the api extra arguments can be given. For example: python api.py --port 5050 --cache The following argument flags exist: Argument Followed by Example Description --host IP address 127.0.0.1 Sets the host IP address --port Port number 5050 Sets the host port --cache Nothing / Enables cahing --debug Nothing / Makes Flask run in Debug mode","title":"api.py"},{"location":"dev_docs/api/#apipy","text":"api.py is the 'head' of the api. This is where all requests come in and are translated from JSON to its internal logic. The api uses Flask .","title":"api.py"},{"location":"dev_docs/api/#the-code","text":"From top to bottom","title":"The code"},{"location":"dev_docs/api/#apiresult","text":"The APIResult dataclass is used as a return value. Functions may return an APIResult where where different return types are possible. @dataclass class APIResult: payload: tuple[int, Operation] | connection | pipelineObject | str = \"\" succes: bool = True HTTPStatusCode:int = 200 payload : The payload is the actual returned value/object. success : Success will be set to False if an exception has occured. In this case the Payload will be a string containing an error message. HTTPStatuscode : Returns what status code the api should respond with. This is 200 by default (OK) and will be changed when an exception occurs.","title":"APIResult"},{"location":"dev_docs/api/#startup-code","text":"This code defines the api and sets it up ready to be started. app = Flask(__name__) CORS(app) swagger = Swagger(app) use_cache = False 'CORS(app)' sets up Cross-Origin Resource Sharing 'swagger = Swagger(app)' sets up swagger 'use_cache = False' makes it so be default the cache will not be used.","title":"Startup code"},{"location":"dev_docs/api/#request-status","text":"This code defines a request that simply returns \"api online\". This is useful for checking if the api is running and able to receive requests. @app.route('/status', methods=['GET']) def get_status(): return jsonify(\"api online\")","title":"request: status"},{"location":"dev_docs/api/#request-datasets","text":"This code defines a request that returns the files that can be loaded. @app.route('/datasets', methods=['GET']) def get_datasets(): return jsonify(loadOperation().loadable_files) It uses a preset variable in the load_operation.py file. This means it will likely need to be rewritten when the workings of loading data is changed from preset datasets to loading from a database. (This is a planned change.)","title":"request: datasets"},{"location":"dev_docs/api/#request-execute-pipeline","text":"This code defines a request to execute a pipeline. @app.route('/execute-pipeline', methods=['POST']) # type: ignore def execute_pipeline(): json = request.get_json(silent=True) request_id = None session_id = None if json is not None: request_id = json.get(\"request_id\") session_id = json.get(\"session_id\") api_res = json_to_pipeline(json) if not api_res.succes: return jsonify_result(api_res.payload, session_id, request_id), api_res.HTTPStatusCode pipobj:pipelineObject = api_res.payload #type: ignore executed = controller.execute_pipeline(pipobj, use_cache, controller.cacheMethod.PIPELINE) if not executed.succes: return jsonify_result(str(f\"There was a problem with a '{executed.operation_of_failure.__class__.__name__}' operation: '\") + str(executed.payload) + \"'\", session_id, request_id), 500 else: json = pipelinememory_to_json(executed.payload) #type: ignore return jsonify_result(json, session_id, request_id), 200","title":"request: Execute-pipeline"},{"location":"dev_docs/api/#-getting-the-json-and-extracting-basic-info","text":"json = request.get_json(silent=True) request_id = None session_id = None if json is not None: request_id = json.get(\"request_id\") session_id = json.get(\"session_id\")","title":"- Getting the JSON and extracting basic info"},{"location":"dev_docs/api/#-calling-json_to_pipeline","text":"api_res = json_to_pipeline(json) if not api_res.succes: return jsonify_result(api_res.payload, session_id, request_id), api_res.HTTPStatusCode pipobj:pipelineObject = api_res.payload #type: ignore If json_to_pipeline(json) was not succesful the exception is returned as response. If it was succesful its return value is saved as pipobj. This will be a pipelineObject.","title":"- calling json_to_pipeline"},{"location":"dev_docs/api/#-executing-pipeline-and-sending-response","text":"executed = controller.execute_pipeline(pipobj, use_cache, controller. cacheMethod.PIPELINE) if not executed.succes: return jsonify_result(str(f\"There was a problem with a '{executed.operation_of_failure.__class__.__name__}' operation: '\") + str(executed.payload) + \"'\", session_id, request_id), 500 else: json = pipelinememory_to_json(executed.payload) #type: ignore return jsonify_result(json, session_id, request_id), 200","title":"- executing pipeline and sending response."},{"location":"dev_docs/api/#json_to_pipeline","text":"Translates JSON to internal logic. def json_to_pipeline(json) -> APIResult: if (json is None): return APIResult(\"No json input was received by api.\", False, 400) if \"operations\" not in json: return APIResult(\"Request does not define operations.\", False, 400) if \"connections\" not in json: return APIResult(\"Request does not define connections.\", False, 400) if \"session_id\" not in json: return APIResult(\"Request does not define sessionid.\", False, 400) json_operations = json[\"operations\"] json_connections = json[\"connections\"] pipobj = pipelineObject() pipobj.session_id = json[\"session_id\"] pipobj.request_id = json.get(\"request_id\") api_res = json_to_operations(pipobj, json_operations) if not api_res.succes: return api_res api_res = json_to_connections(pipobj, json_connections) if not api_res.succes: return api_res return APIResult(pipobj) Some simple checks: if (json is None): return APIResult(\"No json input was received by api.\", False, 400) if \"operations\" not in json: return APIResult(\"Request does not define operations.\", False, 400) if \"connections\" not in json: return APIResult(\"Request does not define connections.\", False, 400) if \"session_id\" not in json: return APIResult(\"Request does not define sessionid.\", False, 400) Defining variables: json_operations = json[\"operations\"] json_connections = json[\"connections\"] pipobj = pipelineObject() pipobj.session_id = json[\"session_id\"] pipobj.request_id = json.get(\"request_id\") Translating operations : api_res = json_to_operations(pipobj, json_operations) if not api_res.succes: return api_res Translating connections : api_res = json_to_connections(pipobj, json_connections) if not api_res.succes: return api_res return APIResult(pipobj)","title":"json_to_pipeline"},{"location":"dev_docs/api/#json-to-operations","text":"It converts a list of JSON operations into a list of operations and adds these to the pipelineObject. This uses operation_to_result def json_to_operations(pipobj:pipelineObject, json_operations:list) -> APIResult: results: list[APIResult] = [] operation_hashes_by_id = {} operations_by_id = {} pipeline:list[Operation] = [] for item in json_operations: api_res = operation_to_result(item) if (api_res.succes): results.append(api_res) else: return api_res for res in results: if type(res.payload) == tuple: payload = res.payload operations_by_id[payload[0]] = payload[1] operation_hashes_by_id[payload[0]] = payload[1].hash() #type: ignore pipeline.append(payload[1]) else: return APIResult(f\"unworkable payload detected while preparing pipeline. Payload should have been tuple(int, Operation). Payload given: {api_res.payload} \", False, 500) pipobj.operations = pipeline pipobj.operations_by_id = operations_by_id pipobj.operation_hashes_by_id = operation_hashes_by_id return APIResult(pipobj)","title":"json to operations"},{"location":"dev_docs/api/#json-to-connections","text":"It converts a list of JSON connections into a list of connections and adds these to the pipelineObject. This uses connection_to_result def json_to_connections(pipobj:pipelineObject, json_connections:list) -> APIResult: results = [] for con in json_connections: api_res = connection_to_result(con, pipobj.operations_by_id) if (api_res.succes): results.append(api_res) else: return api_res connections:list[connection] = [] for res in results: if type(res.payload) == connection: connections.append(res.payload) else: return APIResult(f\"unworkable connection detected while preparing pipeline. Payload should have been connection. Payload given: {api_res.payload}\", False, 500) pipobj.connections = connections return APIResult(pipobj)","title":"json to connections"},{"location":"dev_docs/api/#operation-to-result","text":"It takes an operation defined in JSON and translates it to an operation object of the correct class. It may also perform some checks. def operation_to_result(json_operation: dict[str, str]) -> APIResult: operation:Operation id:int = 0 #get operation match json_operation: case {\"operation\": \"load\"}: params = json_operation.get(\"parameters\") check = check_parameters(params, \"load\") if check.succes == False: return check operation = loadOperation() insertion_result = operation.insert_parameters(params) #type: ignore if type(insertion_result) == Exception: return APIResult(\"A load operation had a problem while inserting parameters. Gave following Exception: \" + str(insertion_result), False, 400) case {\"operation\": \"duplicate\"}: operation = duplicateOperation() case {\"operation\":\"select_columns\"}: operation = selectColumnsOperation() params = json_operation.get(\"parameters\") check = check_parameters(params, \"select_columns\") if check.succes == False: return check insertion_result = operation.insert_parameters(params) #type: ignore if type(insertion_result) == Exception: return APIResult(\"A select_columns operation had a problem while inserting parameters. Gave following Exception: \" + str(insertion_result), False, 400) case {\"operation\":\"merge\"}: params = json_operation.get(\"parameters\") check = check_parameters(params, \"merge\") if check.succes == False: return check operation = mergeOperation() insertion_result = operation.insert_parameters(params) #type: ignore if type(insertion_result) == Exception: return APIResult(\"A merge operation had a problem while inserting parameters. Gave following Exception: \" + str(insertion_result), False, 400) case {\"operation\": \"decision_tree\"}: params = json_operation.get(\"parameters\") check = check_parameters(params, \"decision tree\") if check.succes == False: return check operation = decisionTree() insertion_result = operation.insert_parameters(params) #type: ignore if type(insertion_result) == Exception: return APIResult(\"A decision tree operation had a problem while inserting parameters. Gave following Exception: \" + str(insertion_result), False, 400) case {\"operation\":\"filter_rows\"}: params = json_operation.get(\"parameters\") operation = filterRowsOperation() insertion_result = operation.insert_parameters(params) # type: ignore if type(insertion_result) == Exception: return APIResult(\"A filter rows operation had a problem while inserting parameters. Gave following Exception: \" + str(insertion_result), False, 400) case {\"operation\":\"test_operation\"}: operation = controller.testOperation() case {\"operation\":\"predict\"}: params = json_operation.get(\"parameters\") operation = predict() insertion_result = operation.insert_parameters(params) if type(insertion_result) == Exception: return APIResult(\"A predict operation had a problem while inserting parameters. Gave following Exception: \" + str(insertion_result), False, 400) case {\"operation\": notImpl}: return APIResult(f\"Operation {notImpl} is not implemented\", False, 400) case _: return APIResult(f\"A requested operation is not implemented. Could not find implemented operation regarding the following: {json_operation}\", False, 404) # test id match json_operation: case {\"id\": _id} if type(_id) == int: id = _id case {\"id\": _id}: return APIResult(f\"The following id was found not to be convertible to an integer. id: {_id}.\", False, 400) case _: return APIResult(\"An operation was found not to have an id.\", False, 400) return APIResult(tuple([id, operation])) #type: ignore","title":"operation to result"},{"location":"dev_docs/api/#connection-to-result","text":"It takes a connection defined in JSON and translates it to a connection object. Connectiondict should be the connection in JSON. operations should be a dictionary of operations with their id as the keys. def connection_to_result(connectiondict: dict[str, int], operations:dict[int, Operation]) -> APIResult: return APIResult(connection(operations[connectiondict[\"from\"]], operations[connectiondict[\"to\"]], {cport.FROM:int(connectiondict[\"from_port\"]), cport.TO:int(connectiondict[\"to_port\"])} ))","title":"connection to result"},{"location":"dev_docs/api/#pipelinememory-to-json","text":"This converts a pipelineObject into JSON. This is used to convert internal logic back to JSON so it can be given as response. def pipelinememory_to_json(pipobj:pipelineObject) -> dict[str, str | dict]: pipmem:pipelineMemory = pipobj.pipeline_memory json: dict[str, str | dict] = {} ids_by_operation:dict[Operation, int] = invert_dictionary(pipobj.operations_by_id) for key, value in pipmem.operations_memory.items(): operation:Operation = key[0] port:int = key[1] id:int= ids_by_operation[operation] jsonpipe:dict[str, str] = {} match value: case val if isinstance(val, pd.DataFrame): jsonpipe[\"output\"] = dataframe_to_json(val) jsonpipe[\"output_type\"] = \"data\" case val if isinstance(val, int): jsonpipe[\"output\"] = str(value) jsonpipe[\"output_type\"] = \"integer\" case val if isinstance(val, list): jsonpipe[\"output\"] = str(value) jsonpipe[\"output_type\"] = \"array\" pipe_output = json.get(\"id_\" + str(id)) if type(pipe_output) == dict: pipe_output.update({\"port_\" + str(port): jsonpipe}) else: pipe_output = {\"port_\" + str(port): jsonpipe} json[\"id_\" + str(id)] = pipe_output return json def operation_to_json(operation:Operation): return str(type(operation)) def invert_dictionary(dict:dict) -> dict: return {v: k for k, v in dict.items()}","title":"pipelinememory to JSON"},{"location":"dev_docs/api/#the-start-function","text":"This function starts the api. if __name__ == '__main__': parser = argparse.ArgumentParser(description='Run the Flask application.') parser.add_argument('--host', type=str, default=\"127.0.0.1\", help='The host') parser.add_argument('--port', type=int, default=5000, help='Port number to run the server on') parser.add_argument('--cache', default=False, action='store_true', help='If true, uses redis cache for better performance') parser.add_argument('--debug', default=False, action='store_true', help='Start in debug mode') args = parser.parse_args() use_cache = args.cache app.run(host=args.host,port=args.port, debug=args.debug)","title":"The start function"},{"location":"dev_docs/api/#startup-flags","text":"When starting the api extra arguments can be given. For example: python api.py --port 5050 --cache The following argument flags exist: Argument Followed by Example Description --host IP address 127.0.0.1 Sets the host IP address --port Port number 5050 Sets the host port --cache Nothing / Enables cahing --debug Nothing / Makes Flask run in Debug mode","title":"Startup flags"},{"location":"dev_docs/architecture/","text":"architecture api.py is the income point of the API. It's where the requests arrive. It translates any JSON that coems with the request into internal logic and then gives this to the controller to execute. controller.py and its Controller class are responsible for the delegation of actions to the correct operations. operation_interface.py contains the interface for all operations. Each operation can perform one specific task. An operation can have parameters set to define its behaviour and can take outputs of other operations as input.","title":"architecture"},{"location":"dev_docs/architecture/#architecture","text":"api.py is the income point of the API. It's where the requests arrive. It translates any JSON that coems with the request into internal logic and then gives this to the controller to execute. controller.py and its Controller class are responsible for the delegation of actions to the correct operations. operation_interface.py contains the interface for all operations. Each operation can perform one specific task. An operation can have parameters set to define its behaviour and can take outputs of other operations as input.","title":"architecture"},{"location":"dev_docs/connections/","text":"connections.py cport cport is an enumerator used for increased readability and error safety. It represents the 'in' and 'out' ports of a connection. class cport(Enum): FROM = 0 TO = 1 Connection Read the user documentation about connections A payload can be added to store the output of the 'from_operation'. class connection: from_operation: Operation to_operation: Operation ports: dict[cport, int] payload:input_output_object | None = None def __init__(self, from_op:Operation, to_op:Operation, ports:dict[cport, int]): self.from_operation = from_op self.to_operation = to_op self.ports = ports def add_payload(self, payload): self.payload = payload","title":"connections.py"},{"location":"dev_docs/connections/#connectionspy","text":"","title":"connections.py"},{"location":"dev_docs/connections/#cport","text":"cport is an enumerator used for increased readability and error safety. It represents the 'in' and 'out' ports of a connection. class cport(Enum): FROM = 0 TO = 1","title":"cport"},{"location":"dev_docs/connections/#connection","text":"Read the user documentation about connections A payload can be added to store the output of the 'from_operation'. class connection: from_operation: Operation to_operation: Operation ports: dict[cport, int] payload:input_output_object | None = None def __init__(self, from_op:Operation, to_op:Operation, ports:dict[cport, int]): self.from_operation = from_op self.to_operation = to_op self.ports = ports def add_payload(self, payload): self.payload = payload","title":"Connection"},{"location":"dev_docs/how_to_start/","text":"How to start the API With Docker Run the following command in the root directory (The directory with compose.yml). docker-compose -f compose.yml up Manual startup Open command prompt and move into root folder. Run following command to make a virtual environment. (change venv_name to the name of your virtual environment.) python -m venv venv_name Run this command to start the virtual environment. venv_name\\Scripts\\activate Now run these commands to install dependencies. pip install --no-cache-dir Flask==1.1.4 Flask-CORS Werkzeug MarkupSafe==1.1.1 pip install flasgger==0.9.7.1 pip install pydantic==2.6.3 pip install pandas==1.5.3 pip install seaborn==0.13.2 pip install scikit-learn==1.4.1.post1 pip install pickle-mixin pip install redis==5.0.3 pip install pydataset==0.2.0 pip install tabulate==0.9.0 pip install psycopg2==2.9.5 pip install openpyxl==3.1.2 To run the api run this command. python api.py More startup options","title":"startup"},{"location":"dev_docs/how_to_start/#how-to-start-the-api","text":"","title":"How to start the API"},{"location":"dev_docs/how_to_start/#with-docker","text":"Run the following command in the root directory (The directory with compose.yml). docker-compose -f compose.yml up","title":"With Docker"},{"location":"dev_docs/how_to_start/#manual-startup","text":"Open command prompt and move into root folder. Run following command to make a virtual environment. (change venv_name to the name of your virtual environment.) python -m venv venv_name Run this command to start the virtual environment. venv_name\\Scripts\\activate Now run these commands to install dependencies. pip install --no-cache-dir Flask==1.1.4 Flask-CORS Werkzeug MarkupSafe==1.1.1 pip install flasgger==0.9.7.1 pip install pydantic==2.6.3 pip install pandas==1.5.3 pip install seaborn==0.13.2 pip install scikit-learn==1.4.1.post1 pip install pickle-mixin pip install redis==5.0.3 pip install pydataset==0.2.0 pip install tabulate==0.9.0 pip install psycopg2==2.9.5 pip install openpyxl==3.1.2 To run the api run this command. python api.py More startup options","title":"Manual startup"},{"location":"dev_docs/pipeline/","text":"pipeline.py pipelineObject The purpose of this object is to contain the information needed to execute a pipeline. Such as the operations, connections and a pipelineMemory. class pipelineObject: def __init__(self): self.request_id:str | None= None self.session_id:str = \"\" self.operations:list[Operation] = [] self.connections:list[connection] = [] self.operations_by_id:dict[int, Operation] = {} self.operation_hashes_by_id:dict[int, str] = {} self.pipeline_memory:pipelineMemory = pipelineMemory() pipelineMemory The purpose of this object is to retain and give the return values of executed operations. Return values are saved in a dictionary with as key the operation it comes from and the 'port' through which it was released. class pipelineMemory: def __init__(self) -> None: self.operations_memory: dict[tuple[Operation, int], input_output_object] = {} def get_output(self, operation:Operation, port:int=0): return self.operations_memory.get((operation, port)) def get_outputs(self, operations:tuple[Operation] | None, ports:tuple[int] | None): if operations is None or ports is None: return None return tuple(self.get_output(operation, port) for operation, port in zip(operations, ports)) def add_output(self, operation:Operation, port:int, iuo:input_output_object): self.operations_memory[operation, port] = iuo def get_full_memory(self): return self.operations_memory","title":"pipeline.py"},{"location":"dev_docs/pipeline/#pipelinepy","text":"","title":"pipeline.py"},{"location":"dev_docs/pipeline/#pipelineobject","text":"The purpose of this object is to contain the information needed to execute a pipeline. Such as the operations, connections and a pipelineMemory. class pipelineObject: def __init__(self): self.request_id:str | None= None self.session_id:str = \"\" self.operations:list[Operation] = [] self.connections:list[connection] = [] self.operations_by_id:dict[int, Operation] = {} self.operation_hashes_by_id:dict[int, str] = {} self.pipeline_memory:pipelineMemory = pipelineMemory()","title":"pipelineObject"},{"location":"dev_docs/pipeline/#pipelinememory","text":"The purpose of this object is to retain and give the return values of executed operations. Return values are saved in a dictionary with as key the operation it comes from and the 'port' through which it was released. class pipelineMemory: def __init__(self) -> None: self.operations_memory: dict[tuple[Operation, int], input_output_object] = {} def get_output(self, operation:Operation, port:int=0): return self.operations_memory.get((operation, port)) def get_outputs(self, operations:tuple[Operation] | None, ports:tuple[int] | None): if operations is None or ports is None: return None return tuple(self.get_output(operation, port) for operation, port in zip(operations, ports)) def add_output(self, operation:Operation, port:int, iuo:input_output_object): self.operations_memory[operation, port] = iuo def get_full_memory(self): return self.operations_memory","title":"pipelineMemory"},{"location":"dev_docs/operations/aggregate_operation/","text":"Aggregate Operation The aggregate_operation is used for aggregating data as in SQL Read the user documentation about the aggregate_operation class aggregateOperation(oi.operation_interface): init group_by_column defines what column to group by. aggregate_column defines what column to aggregate on agg_method defines what method to aggregate with. def __init__(self): self.group_by_column: str self.aggregate_column: str self.agg_method: aggregate_method insert_parameters def insert_parameters(self, params:dict) -> Exception | None: if params is None: return Exception(\"parameters missing\") gcol = params.get(\"group_by\") if gcol is None: return Exception(\"parameter 'group_by' missing\") self.group_by_column = gcol agcol = params.get(\"agg_column\") if agcol is None: return Exception(\"parameter 'agg_column' missing\") self.aggregate_column = agcol agmeth = self.to_aggregate_method(params.get(\"agg_method\")) if isinstance(agmeth, Exception): return agmeth self.agg_method = agmeth call def __call__(self, inputs): for con in inputs: if isinstance(con.payload, pd.DataFrame): df = con.payload.copy() aggregated_data = df .groupby(self.group_by_column) .aggregate({self.aggregate_column:self.agg_method.value}) return [{\"port\":0, \"output\":aggregated_data}] to_aggregate_method This method converts a string to an aggregate_method, if possible. def to_aggregate_method(self, param): for method in aggregate_method: if method.value == param: return method return Exception(\"Invalid or missing agg_method\") aggregate_method Enumerator used for increased readability and error safety. Defines the possible ways to aggregate. class aggregate_method(Enum): COUNT = 'count' MIN = 'min' MAX = 'max' AVG = 'avg' SUM = 'sum'","title":"aggregate_operation.py"},{"location":"dev_docs/operations/aggregate_operation/#aggregate-operation","text":"The aggregate_operation is used for aggregating data as in SQL Read the user documentation about the aggregate_operation class aggregateOperation(oi.operation_interface):","title":"Aggregate Operation"},{"location":"dev_docs/operations/aggregate_operation/#init","text":"group_by_column defines what column to group by. aggregate_column defines what column to aggregate on agg_method defines what method to aggregate with. def __init__(self): self.group_by_column: str self.aggregate_column: str self.agg_method: aggregate_method","title":"init"},{"location":"dev_docs/operations/aggregate_operation/#insert_parameters","text":"def insert_parameters(self, params:dict) -> Exception | None: if params is None: return Exception(\"parameters missing\") gcol = params.get(\"group_by\") if gcol is None: return Exception(\"parameter 'group_by' missing\") self.group_by_column = gcol agcol = params.get(\"agg_column\") if agcol is None: return Exception(\"parameter 'agg_column' missing\") self.aggregate_column = agcol agmeth = self.to_aggregate_method(params.get(\"agg_method\")) if isinstance(agmeth, Exception): return agmeth self.agg_method = agmeth","title":"insert_parameters"},{"location":"dev_docs/operations/aggregate_operation/#call","text":"def __call__(self, inputs): for con in inputs: if isinstance(con.payload, pd.DataFrame): df = con.payload.copy() aggregated_data = df .groupby(self.group_by_column) .aggregate({self.aggregate_column:self.agg_method.value}) return [{\"port\":0, \"output\":aggregated_data}]","title":"call"},{"location":"dev_docs/operations/aggregate_operation/#to_aggregate_method","text":"This method converts a string to an aggregate_method, if possible. def to_aggregate_method(self, param): for method in aggregate_method: if method.value == param: return method return Exception(\"Invalid or missing agg_method\")","title":"to_aggregate_method"},{"location":"dev_docs/operations/aggregate_operation/#aggregate_method","text":"Enumerator used for increased readability and error safety. Defines the possible ways to aggregate. class aggregate_method(Enum): COUNT = 'count' MIN = 'min' MAX = 'max' AVG = 'avg' SUM = 'sum'","title":"aggregate_method"},{"location":"dev_docs/operations/filter_rows_operation/","text":"filter_rows_operation Read the user documentation about the filter_rows_operation accessory classes comparator Used for comparisons. class Comparator(Enum): EQUAL = '==' NOT_EQUAL = '!=' GREATER_THAN = '>' LESS_THAN = '<' GREATER_EQUAL_THAN = '>=' LESS_EQUAL_THAN = '<=' def compare(self, x, y): return eval(f\"{x} {self.value} {y}\") comparee Compare_value is used when comparing rows in a dataframe with a static value. Compare_column is used when comparing one row with another. class compare_column(): def __init__(self): self.column: str class compare_value(): def __init__(self): self.value: str | int | float Comparee = compare_value | compare_column filterRowsOperation class filterRowsOperation(oi.operation_interface): init conditions has the list with conditions to check if rows adhere to. include decides whether to keep or get rid of the rows that comply with the conditions. set can either be set_type.INTERSECT or set_type.UNION . In the case it is set_type.INTERSECT only the rows that comply with ALL conditions are kept/removed. In the case it is set_type.UNION only the rows that comply with AT LEAST ONE condition are kept/removed. def __init__(self): self.conditions:list[Condition] self.include:bool = True self.set:set_type = set_type.INTERSECT hash def hash(self, inputs: list[connection]=[], hash = None) -> str: return super().hash(inputs, hash) insert_parameters def insert_parameters(self, params:dict) -> Exception | None: self.conditions = [] if params.get(\"conditions\") is None: return Exception(\"No conditions were given.\") paramconditions:list = params.get(\"conditions\") # type: ignore for paramcond in paramconditions: column = paramcond.get(\"column\") comparee_val = paramcond.get(\"compare_with\") if paramcond.get(\"compare_with_type\") == \"value\": comparee = compare_value() comparee.value = comparee_val elif paramcond.get(\"compare_with_type\") == \"column\": if not isinstance(comparee_val, str): return Exception(\"'compare_with' should be a string to be used as a column.\") comparee = compare_column() comparee.column = comparee_val else: return Exception(\"Invalid value for 'compare with type'\") comparator:Comparator result = self.get_comparator(paramcond.get(\"comparator\")) if isinstance(result, Exception): return result comparator = result condition = Condition() condition.insert_vars(column, comparator, comparee) self.conditions.append(condition) if params.get(\"include\") is not None: self.include = bool(params.get(\"include\")) else: self.include = True set_operation = params.get(\"set_operation\") if set_operation is not None: if set_operation == \"intersect\": self.set = set_type.INTERSECT if set_operation == \"union\": self.set = set_type.UNION else: self.set = set_type.INTERSECT call def __call__(self, inputs: list[connection]=[]): for con in inputs: if isinstance(con.payload, pd.DataFrame): df:pd.DataFrame = con.payload.copy() if self.set == set_type.INTERSECT: for condition in self.conditions: df = condition(df) # type: ignore if self.set == set_type.UNION: conditioned_dataframes = [] for condition in self.conditions: conditioned_dataframes.append(condition(df)) # type: ignore df = pd.concat(conditioned_dataframes) if not self.include: df_exclude = con.payload.copy() merged_df = pd.merge(df_exclude, df, on=list(df.columns), how='left', indicator=True) df_exclude = merged_df[merged_df['_merge'] == 'left_only'].drop(columns='_merge') df = df_exclude return [{\"port\":0, \"output\": df}] return Exception(\"No dataframe was found in inputs\") get_comparator This takes a string and returns the corresponding Comparator def get_comparator(self, paramcomparator:str) -> Comparator | Exception: for comp in Comparator: if comp.value == paramcomparator: return comp return Exception(\"Invalid comparator\")","title":"filter_rows_operation.py"},{"location":"dev_docs/operations/filter_rows_operation/#filter_rows_operation","text":"Read the user documentation about the filter_rows_operation","title":"filter_rows_operation"},{"location":"dev_docs/operations/filter_rows_operation/#accessory-classes","text":"","title":"accessory classes"},{"location":"dev_docs/operations/filter_rows_operation/#comparator","text":"Used for comparisons. class Comparator(Enum): EQUAL = '==' NOT_EQUAL = '!=' GREATER_THAN = '>' LESS_THAN = '<' GREATER_EQUAL_THAN = '>=' LESS_EQUAL_THAN = '<=' def compare(self, x, y): return eval(f\"{x} {self.value} {y}\")","title":"comparator"},{"location":"dev_docs/operations/filter_rows_operation/#comparee","text":"Compare_value is used when comparing rows in a dataframe with a static value. Compare_column is used when comparing one row with another. class compare_column(): def __init__(self): self.column: str class compare_value(): def __init__(self): self.value: str | int | float Comparee = compare_value | compare_column","title":"comparee"},{"location":"dev_docs/operations/filter_rows_operation/#filterrowsoperation","text":"class filterRowsOperation(oi.operation_interface):","title":"filterRowsOperation"},{"location":"dev_docs/operations/filter_rows_operation/#init","text":"conditions has the list with conditions to check if rows adhere to. include decides whether to keep or get rid of the rows that comply with the conditions. set can either be set_type.INTERSECT or set_type.UNION . In the case it is set_type.INTERSECT only the rows that comply with ALL conditions are kept/removed. In the case it is set_type.UNION only the rows that comply with AT LEAST ONE condition are kept/removed. def __init__(self): self.conditions:list[Condition] self.include:bool = True self.set:set_type = set_type.INTERSECT","title":"init"},{"location":"dev_docs/operations/filter_rows_operation/#hash","text":"def hash(self, inputs: list[connection]=[], hash = None) -> str: return super().hash(inputs, hash)","title":"hash"},{"location":"dev_docs/operations/filter_rows_operation/#insert_parameters","text":"def insert_parameters(self, params:dict) -> Exception | None: self.conditions = [] if params.get(\"conditions\") is None: return Exception(\"No conditions were given.\") paramconditions:list = params.get(\"conditions\") # type: ignore for paramcond in paramconditions: column = paramcond.get(\"column\") comparee_val = paramcond.get(\"compare_with\") if paramcond.get(\"compare_with_type\") == \"value\": comparee = compare_value() comparee.value = comparee_val elif paramcond.get(\"compare_with_type\") == \"column\": if not isinstance(comparee_val, str): return Exception(\"'compare_with' should be a string to be used as a column.\") comparee = compare_column() comparee.column = comparee_val else: return Exception(\"Invalid value for 'compare with type'\") comparator:Comparator result = self.get_comparator(paramcond.get(\"comparator\")) if isinstance(result, Exception): return result comparator = result condition = Condition() condition.insert_vars(column, comparator, comparee) self.conditions.append(condition) if params.get(\"include\") is not None: self.include = bool(params.get(\"include\")) else: self.include = True set_operation = params.get(\"set_operation\") if set_operation is not None: if set_operation == \"intersect\": self.set = set_type.INTERSECT if set_operation == \"union\": self.set = set_type.UNION else: self.set = set_type.INTERSECT","title":"insert_parameters"},{"location":"dev_docs/operations/filter_rows_operation/#call","text":"def __call__(self, inputs: list[connection]=[]): for con in inputs: if isinstance(con.payload, pd.DataFrame): df:pd.DataFrame = con.payload.copy() if self.set == set_type.INTERSECT: for condition in self.conditions: df = condition(df) # type: ignore if self.set == set_type.UNION: conditioned_dataframes = [] for condition in self.conditions: conditioned_dataframes.append(condition(df)) # type: ignore df = pd.concat(conditioned_dataframes) if not self.include: df_exclude = con.payload.copy() merged_df = pd.merge(df_exclude, df, on=list(df.columns), how='left', indicator=True) df_exclude = merged_df[merged_df['_merge'] == 'left_only'].drop(columns='_merge') df = df_exclude return [{\"port\":0, \"output\": df}] return Exception(\"No dataframe was found in inputs\")","title":"call"},{"location":"dev_docs/operations/filter_rows_operation/#get_comparator","text":"This takes a string and returns the corresponding Comparator def get_comparator(self, paramcomparator:str) -> Comparator | Exception: for comp in Comparator: if comp.value == paramcomparator: return comp return Exception(\"Invalid comparator\")","title":"get_comparator"},{"location":"dev_docs/operations/load_operation/","text":"load_operation The load_operation is used for loading data and gevis them as pandas dataframes. Read the user documentation about the load_operation class loadOperation(oi.operation_interface): init base_sample_size is the default amount of rows given if sample is True. sample : If sample is False, it does nothing. If it is True, load_operation will only return the amount of rows defined in base_sample_size . If it is an integer, load_operation will return that amount of rows. filename defines the name of the file to be loaded in. randomize_sample : If True will randomize what rows are returned if only a sample is returned. If False, load_operation will consistently return the first x rows. loadable_files defines what files the load_operation has access to. Filenames not in this list will never be loaded. def __init__(self): self.base_sample_size = 25 self.filename:str self.sample:bool | int self.randomize_sample:bool = True self.loadable_files = ['mtcars', 'titanic', 'iris','student-attitude'] hash def hash(self, inputs: list[connection]=[], hash = None) -> str: if hash is None: hash = hashlib.md5() hash.update(bytes(self.filename, 'utf-8')) if type(self.sample) is bool: hash.update(bytes(str(int(self.sample == True)), 'utf-8')) else: hash.update(bytes(str(self.sample), 'utf-8')) return super().hash(inputs, hash) insert_parameters def insert_parameters(self, params:dict) -> Exception | None: if params is None: return Exception(\"parameters missing\") if params.get(\"filename\") is None: return Exception(\"parameter 'filename' missing\") else: self.filename = params.get(\"filename\") #type: ignore self.sample = string_to_bool_or_int(params.get(\"sample\", False)) if type(self.sample) == bool: if self.sample == True: self.sample = self.base_sample_size else: self.sample = -1 if params.get(\"randomize_sample\") is not None: self.randomize_sample = bool(params.get(\"randomize_sample\")) call def __call__(self, inputs): return [{\"port\":0, \"output\":self.load_data(self.filename, self.sample)}] load_data This function loads in data as a dataframe. This function is used by 'call'. A filename that can not be found will resutl in an Exception. def load_data(self, filename:str, sample_size:int) -> pd.DataFrame | ValueError: if filename in self.loadable_files: data:pd.DataFrame = pd.DataFrame(pydata(filename)) if sample_size > -1: if self.randomize_sample: data = data.sample(n=sample_size) else: data = data.head(sample_size) return data else: return ValueError(f\"Invalid dataset name. Choose from {self.loadable_files}\")","title":"load_operation.py"},{"location":"dev_docs/operations/load_operation/#load_operation","text":"The load_operation is used for loading data and gevis them as pandas dataframes. Read the user documentation about the load_operation class loadOperation(oi.operation_interface):","title":"load_operation"},{"location":"dev_docs/operations/load_operation/#init","text":"base_sample_size is the default amount of rows given if sample is True. sample : If sample is False, it does nothing. If it is True, load_operation will only return the amount of rows defined in base_sample_size . If it is an integer, load_operation will return that amount of rows. filename defines the name of the file to be loaded in. randomize_sample : If True will randomize what rows are returned if only a sample is returned. If False, load_operation will consistently return the first x rows. loadable_files defines what files the load_operation has access to. Filenames not in this list will never be loaded. def __init__(self): self.base_sample_size = 25 self.filename:str self.sample:bool | int self.randomize_sample:bool = True self.loadable_files = ['mtcars', 'titanic', 'iris','student-attitude']","title":"init"},{"location":"dev_docs/operations/load_operation/#hash","text":"def hash(self, inputs: list[connection]=[], hash = None) -> str: if hash is None: hash = hashlib.md5() hash.update(bytes(self.filename, 'utf-8')) if type(self.sample) is bool: hash.update(bytes(str(int(self.sample == True)), 'utf-8')) else: hash.update(bytes(str(self.sample), 'utf-8')) return super().hash(inputs, hash)","title":"hash"},{"location":"dev_docs/operations/load_operation/#insert_parameters","text":"def insert_parameters(self, params:dict) -> Exception | None: if params is None: return Exception(\"parameters missing\") if params.get(\"filename\") is None: return Exception(\"parameter 'filename' missing\") else: self.filename = params.get(\"filename\") #type: ignore self.sample = string_to_bool_or_int(params.get(\"sample\", False)) if type(self.sample) == bool: if self.sample == True: self.sample = self.base_sample_size else: self.sample = -1 if params.get(\"randomize_sample\") is not None: self.randomize_sample = bool(params.get(\"randomize_sample\"))","title":"insert_parameters"},{"location":"dev_docs/operations/load_operation/#call","text":"def __call__(self, inputs): return [{\"port\":0, \"output\":self.load_data(self.filename, self.sample)}]","title":"call"},{"location":"dev_docs/operations/load_operation/#load_data","text":"This function loads in data as a dataframe. This function is used by 'call'. A filename that can not be found will resutl in an Exception. def load_data(self, filename:str, sample_size:int) -> pd.DataFrame | ValueError: if filename in self.loadable_files: data:pd.DataFrame = pd.DataFrame(pydata(filename)) if sample_size > -1: if self.randomize_sample: data = data.sample(n=sample_size) else: data = data.head(sample_size) return data else: return ValueError(f\"Invalid dataset name. Choose from {self.loadable_files}\")","title":"load_data"},{"location":"dev_docs/operations/operation_blueprint/","text":"blueprint This is the general structure that all operations follow. Examples are from the load_operation init The init function defines the variables and their types. Default values may also be defined. Example: def __init__(self): self.base_sample_size = 25 self.filename:str self.sample:bool | int self.randomize_sample:bool = True self.loadable_files = ['mtcars', 'titanic', 'iris','student-attitude'] hash This function is important for caching. The hash function calculates a hash using set variables and a list of inputs if given. The inputs are given as a list of connections . The hash is calculated based on what is in the payloads of these connections. Example: def hash(self, inputs: list[connection]=[], hash = None) -> str: if hash is None: hash = hashlib.md5() hash.update(bytes(self.filename, 'utf-8')) if type(self.sample) is bool: hash.update(bytes(str(int(self.sample == True)), 'utf-8')) else: hash.update(bytes(str(self.sample), 'utf-8')) return super().hash(inputs, hash) insert_parameters This function inserts parameters into the variables of the operation. These are to be given as a dictionary with as key a string representing the variable and as value the value to be inserted. The keys preferably have the same name as the variables they represent. Example: def insert_parameters(self, params:dict) -> Exception | None: if params is None: return Exception(\"parameters missing\") if params.get(\"filename\") is None: return Exception(\"parameter 'filename' missing\") else: self.filename = params.get(\"filename\") #type: ignore self.sample = string_to_bool_or_int(params.get(\"sample\", False)) if type(self.sample) == bool: if self.sample == True: self.sample = self.base_sample_size else: self.sample = -1 if params.get(\"randomize_sample\") is not None: self.randomize_sample = bool(params.get(\"randomize_sample\")) call This is how the operations are executed. It always accepts inputs but does not necessarily use them. It returns a list of outputs in the form of dictionaries with the port on which the output is given and the actual object or dataframe it outputs. Example: def __call__(self, inputs): return [{\"port\":0, \"output\":self.load_data(self.filename, self.sample)}] In this case it calls upon another function in the object to determine the output. others Other functions can be implemented to make the code mroe readable. These should be considered private functions. Example: def load_data(self, filename:str, sample_size:int) -> pd.DataFrame | ValueError: if filename in self.loadable_files: # change this later if filename == \"student-attitude\": data:pd.DataFrame = pd.read_excel(os.path.join(os.path.dirname(__file__), 'extra_datasets', 'attitudereformat.xlsx')) else: data:pd.DataFrame = pd.DataFrame(pydata(filename)) if sample_size > -1: if self.randomize_sample: data = data.sample(n=sample_size) else: data = data.head(sample_size) return data else: return ValueError(f\"Invalid dataset name. Choose from {self.loadable_files}\")","title":"operation blueprint"},{"location":"dev_docs/operations/operation_blueprint/#blueprint","text":"This is the general structure that all operations follow. Examples are from the load_operation","title":"blueprint"},{"location":"dev_docs/operations/operation_blueprint/#init","text":"The init function defines the variables and their types. Default values may also be defined. Example: def __init__(self): self.base_sample_size = 25 self.filename:str self.sample:bool | int self.randomize_sample:bool = True self.loadable_files = ['mtcars', 'titanic', 'iris','student-attitude']","title":"init"},{"location":"dev_docs/operations/operation_blueprint/#hash","text":"This function is important for caching. The hash function calculates a hash using set variables and a list of inputs if given. The inputs are given as a list of connections . The hash is calculated based on what is in the payloads of these connections. Example: def hash(self, inputs: list[connection]=[], hash = None) -> str: if hash is None: hash = hashlib.md5() hash.update(bytes(self.filename, 'utf-8')) if type(self.sample) is bool: hash.update(bytes(str(int(self.sample == True)), 'utf-8')) else: hash.update(bytes(str(self.sample), 'utf-8')) return super().hash(inputs, hash)","title":"hash"},{"location":"dev_docs/operations/operation_blueprint/#insert_parameters","text":"This function inserts parameters into the variables of the operation. These are to be given as a dictionary with as key a string representing the variable and as value the value to be inserted. The keys preferably have the same name as the variables they represent. Example: def insert_parameters(self, params:dict) -> Exception | None: if params is None: return Exception(\"parameters missing\") if params.get(\"filename\") is None: return Exception(\"parameter 'filename' missing\") else: self.filename = params.get(\"filename\") #type: ignore self.sample = string_to_bool_or_int(params.get(\"sample\", False)) if type(self.sample) == bool: if self.sample == True: self.sample = self.base_sample_size else: self.sample = -1 if params.get(\"randomize_sample\") is not None: self.randomize_sample = bool(params.get(\"randomize_sample\"))","title":"insert_parameters"},{"location":"dev_docs/operations/operation_blueprint/#call","text":"This is how the operations are executed. It always accepts inputs but does not necessarily use them. It returns a list of outputs in the form of dictionaries with the port on which the output is given and the actual object or dataframe it outputs. Example: def __call__(self, inputs): return [{\"port\":0, \"output\":self.load_data(self.filename, self.sample)}] In this case it calls upon another function in the object to determine the output.","title":"call"},{"location":"dev_docs/operations/operation_blueprint/#others","text":"Other functions can be implemented to make the code mroe readable. These should be considered private functions. Example: def load_data(self, filename:str, sample_size:int) -> pd.DataFrame | ValueError: if filename in self.loadable_files: # change this later if filename == \"student-attitude\": data:pd.DataFrame = pd.read_excel(os.path.join(os.path.dirname(__file__), 'extra_datasets', 'attitudereformat.xlsx')) else: data:pd.DataFrame = pd.DataFrame(pydata(filename)) if sample_size > -1: if self.randomize_sample: data = data.sample(n=sample_size) else: data = data.head(sample_size) return data else: return ValueError(f\"Invalid dataset name. Choose from {self.loadable_files}\")","title":"others"},{"location":"dev_docs/operations/select_columns_operation/","text":"select_columns_operation The select_columns_operation is used for filtering out unneeded columns. Read the user documentation about the select_columns_operation class selectColumnsOperation(oi.operation_interface): init columns contains the list of columns (by column name) that are to be selected. def __init__(self): self.columns:list = [] hash def hash(self, inputs: list[connection]=[], hash = None) -> str: if hash is None: hash = hashlib.md5() cols = sorted(self.columns) for col in cols: hash.update(bytes(col, 'utf-8')) return super().hash(inputs, hash) insert_parameters def insert_parameters(self, params:dict) -> Exception | None: if params is None: return Exception(\"parameters missing\") if params.get(\"columns\") is None: return Exception(\"parameter 'columns' missing\") elif not isinstance(params.get(\"columns\"), list) : return Exception(\"parameter 'columns' must be array\") else: self.columns = params.get(\"columns\") #type: ignore call Included columns go to port 0. Excluded columns go to port 1. def __call__(self, inputs: list[connection]=[]): for con in inputs: if isinstance(con.payload, pd.DataFrame): return [ {\"port\":0, \"output\": self.select_columns(con.payload, self.columns, include_columns=True)}, {\"port\":1, \"output\": self.select_columns(con.payload, self.columns, include_columns=False)} ] return Exception(\"No dataframe was found in inputs\") select_columns def select_columns(self, dataset: pd.DataFrame, col_names_list: list[str], include_columns: bool = True) -> pd.DataFrame | Exception: dataset_cols = dataset.columns.tolist() for col in col_names_list: if col not in dataset_cols: return Exception(\"A column to select was not found in the dataset\") # Trim whitespace from each column name col_names_list = [col.strip() for col in col_names_list] # Filter columns based on the Include_Exclude parameter if include_columns: filtered_dataset = dataset[col_names_list] else: #(exclude columns) filtered_dataset = dataset.drop(columns=col_names_list) return filtered_dataset","title":"select_columns_operation.py"},{"location":"dev_docs/operations/select_columns_operation/#select_columns_operation","text":"The select_columns_operation is used for filtering out unneeded columns. Read the user documentation about the select_columns_operation class selectColumnsOperation(oi.operation_interface):","title":"select_columns_operation"},{"location":"dev_docs/operations/select_columns_operation/#init","text":"columns contains the list of columns (by column name) that are to be selected. def __init__(self): self.columns:list = []","title":"init"},{"location":"dev_docs/operations/select_columns_operation/#hash","text":"def hash(self, inputs: list[connection]=[], hash = None) -> str: if hash is None: hash = hashlib.md5() cols = sorted(self.columns) for col in cols: hash.update(bytes(col, 'utf-8')) return super().hash(inputs, hash)","title":"hash"},{"location":"dev_docs/operations/select_columns_operation/#insert_parameters","text":"def insert_parameters(self, params:dict) -> Exception | None: if params is None: return Exception(\"parameters missing\") if params.get(\"columns\") is None: return Exception(\"parameter 'columns' missing\") elif not isinstance(params.get(\"columns\"), list) : return Exception(\"parameter 'columns' must be array\") else: self.columns = params.get(\"columns\") #type: ignore","title":"insert_parameters"},{"location":"dev_docs/operations/select_columns_operation/#call","text":"Included columns go to port 0. Excluded columns go to port 1. def __call__(self, inputs: list[connection]=[]): for con in inputs: if isinstance(con.payload, pd.DataFrame): return [ {\"port\":0, \"output\": self.select_columns(con.payload, self.columns, include_columns=True)}, {\"port\":1, \"output\": self.select_columns(con.payload, self.columns, include_columns=False)} ] return Exception(\"No dataframe was found in inputs\")","title":"call"},{"location":"dev_docs/operations/select_columns_operation/#select_columns","text":"def select_columns(self, dataset: pd.DataFrame, col_names_list: list[str], include_columns: bool = True) -> pd.DataFrame | Exception: dataset_cols = dataset.columns.tolist() for col in col_names_list: if col not in dataset_cols: return Exception(\"A column to select was not found in the dataset\") # Trim whitespace from each column name col_names_list = [col.strip() for col in col_names_list] # Filter columns based on the Include_Exclude parameter if include_columns: filtered_dataset = dataset[col_names_list] else: #(exclude columns) filtered_dataset = dataset.drop(columns=col_names_list) return filtered_dataset","title":"select_columns"},{"location":"user_docs/Execute-pipeline/","text":"Pipelines What is a pipeline? A Pipeline is a list of things you want to do ( operations ) and the order you want them done ( connections ). For example, you might want to retrieve data from a database, select a few columns of data and then train a linear regression model on it. In this case you would use 3 operations and define connections between these operations. Execute Pipeline You can execute a pipeline by sending a '/Execute-pipeline' POST request to the API. What goes in? In order to execute a pipeline you will need to defien the pipeline in JSON. A pipeline is defined as follows: { \"session_id\":session_id, \"request_id\":request_id, \"connections\":[ connection_1, connection_2, ... ], \"operations\":[ operation_1, operation_2, ... ]} Example: { \"session_id\":1, \"request_id\":1, \"connections\":[ {\"from\":1, \"to\":2, \"from_port\":0, \"to_port\":0} ], \"operations\":[ {\"id\":1,\"operation\":\"load\",\"parameters\":{\"filename\":\"titanic\", \"sample\":1}}, {\"id\":2,\"operation\":\"select_columns\", \"parameters\":{\"columns\":[\"age\", \"class\"]}} ]} What comes out? The response will return a JSON dictionary With the session_id, the request_id and the results of the executed pipeline. The result will be a dictionary with the id's of the operations. The id's will have a dictionary with the ports they outputted to. The ports will have a dictionary with output and output type. Example: { \"request_id\": 1, \"result\": { \"id_1\": { \"port_0\": { \"output\": \"{\\\"columns\\\": [\\\"index\\\", \\\"class\\\", \\\"age\\\", \\\"sex\\\", \\\"survived\\\"], \\\"datatypes\\\": {\\\"class\\\": \\\"object\\\", \\\"age\\\": \\\"object\\\", \\\"sex\\\": \\\"object\\\", \\\"survived\\\": \\\"object\\\"}, \\\"data\\\": [[0, \\\"1st class\\\", \\\"adults\\\", \\\"man\\\", \\\"yes\\\", 1]]}\", \"output_type\": \"data\" } }, \"id_2\": { \"port_0\": { \"output\": \"{\\\"columns\\\": [\\\"index\\\", \\\"age\\\", \\\"class\\\"], \\\"datatypes\\\": {\\\"age\\\": \\\"object\\\", \\\"class\\\": \\\"object\\\"}, \\\"data\\\": [[0, \\\"adults\\\", \\\"1st class\\\", 1]]}\", \"output_type\": \"data\" }, \"port_1\": { \"output\": \"{\\\"columns\\\": [\\\"index\\\", \\\"sex\\\", \\\"survived\\\"], \\\"datatypes\\\": {\\\"sex\\\": \\\"object\\\", \\\"survived\\\": \\\"object\\\"}, \\\"data\\\": [[0, \\\"man\\\", \\\"yes\\\", 1]]}\", \"output_type\": \"data\" } } }, \"session_id\": 1 } To get the output of operation with id 3 from port 0 you would call: json[\"result\"][\"id_3\"][\"port_0\"][\"output\"] Errors","title":"Pipelines"},{"location":"user_docs/Execute-pipeline/#pipelines","text":"","title":"Pipelines"},{"location":"user_docs/Execute-pipeline/#what-is-a-pipeline","text":"A Pipeline is a list of things you want to do ( operations ) and the order you want them done ( connections ). For example, you might want to retrieve data from a database, select a few columns of data and then train a linear regression model on it. In this case you would use 3 operations and define connections between these operations.","title":"What is a pipeline?"},{"location":"user_docs/Execute-pipeline/#execute-pipeline","text":"You can execute a pipeline by sending a '/Execute-pipeline' POST request to the API.","title":"Execute Pipeline"},{"location":"user_docs/Execute-pipeline/#what-goes-in","text":"In order to execute a pipeline you will need to defien the pipeline in JSON. A pipeline is defined as follows: { \"session_id\":session_id, \"request_id\":request_id, \"connections\":[ connection_1, connection_2, ... ], \"operations\":[ operation_1, operation_2, ... ]} Example: { \"session_id\":1, \"request_id\":1, \"connections\":[ {\"from\":1, \"to\":2, \"from_port\":0, \"to_port\":0} ], \"operations\":[ {\"id\":1,\"operation\":\"load\",\"parameters\":{\"filename\":\"titanic\", \"sample\":1}}, {\"id\":2,\"operation\":\"select_columns\", \"parameters\":{\"columns\":[\"age\", \"class\"]}} ]}","title":"What goes in?"},{"location":"user_docs/Execute-pipeline/#what-comes-out","text":"The response will return a JSON dictionary With the session_id, the request_id and the results of the executed pipeline. The result will be a dictionary with the id's of the operations. The id's will have a dictionary with the ports they outputted to. The ports will have a dictionary with output and output type. Example: { \"request_id\": 1, \"result\": { \"id_1\": { \"port_0\": { \"output\": \"{\\\"columns\\\": [\\\"index\\\", \\\"class\\\", \\\"age\\\", \\\"sex\\\", \\\"survived\\\"], \\\"datatypes\\\": {\\\"class\\\": \\\"object\\\", \\\"age\\\": \\\"object\\\", \\\"sex\\\": \\\"object\\\", \\\"survived\\\": \\\"object\\\"}, \\\"data\\\": [[0, \\\"1st class\\\", \\\"adults\\\", \\\"man\\\", \\\"yes\\\", 1]]}\", \"output_type\": \"data\" } }, \"id_2\": { \"port_0\": { \"output\": \"{\\\"columns\\\": [\\\"index\\\", \\\"age\\\", \\\"class\\\"], \\\"datatypes\\\": {\\\"age\\\": \\\"object\\\", \\\"class\\\": \\\"object\\\"}, \\\"data\\\": [[0, \\\"adults\\\", \\\"1st class\\\", 1]]}\", \"output_type\": \"data\" }, \"port_1\": { \"output\": \"{\\\"columns\\\": [\\\"index\\\", \\\"sex\\\", \\\"survived\\\"], \\\"datatypes\\\": {\\\"sex\\\": \\\"object\\\", \\\"survived\\\": \\\"object\\\"}, \\\"data\\\": [[0, \\\"man\\\", \\\"yes\\\", 1]]}\", \"output_type\": \"data\" } } }, \"session_id\": 1 } To get the output of operation with id 3 from port 0 you would call: json[\"result\"][\"id_3\"][\"port_0\"][\"output\"]","title":"What comes out?"},{"location":"user_docs/Execute-pipeline/#errors","text":"","title":"Errors"},{"location":"user_docs/connections_info/","text":"Connections What is a connection? A connection describes a dataflow from one operation to another. For example: a connection from a load operation to a select_columns operation will send the data retrieved in the load operation to the select_columns operation so it can perform its action on that data. Defining a connection A connection is defined in JSON as follows: {\"from\":operation_id, \"to\":operation_id, \"from_port\":port_nr, \"to_port\":port_nr} Example: {\"from\":1, \"to\":2, \"from_port\":0, \"to_port\":0} It is a dictionary that defines a 'from' operation where the data comes from and a 'to' operation where the data goes to. here you fill in the id's you used when defining the operations. a 'from_port'. It also defines a 'from_port' and a 'to_port' that describe from which port in the one operation to which port in the other operation the data needs to go.","title":"connections"},{"location":"user_docs/connections_info/#connections","text":"","title":"Connections"},{"location":"user_docs/connections_info/#what-is-a-connection","text":"A connection describes a dataflow from one operation to another. For example: a connection from a load operation to a select_columns operation will send the data retrieved in the load operation to the select_columns operation so it can perform its action on that data.","title":"What is a connection?"},{"location":"user_docs/connections_info/#defining-a-connection","text":"A connection is defined in JSON as follows: {\"from\":operation_id, \"to\":operation_id, \"from_port\":port_nr, \"to_port\":port_nr} Example: {\"from\":1, \"to\":2, \"from_port\":0, \"to_port\":0} It is a dictionary that defines a 'from' operation where the data comes from and a 'to' operation where the data goes to. here you fill in the id's you used when defining the operations. a 'from_port'. It also defines a 'from_port' and a 'to_port' that describe from which port in the one operation to which port in the other operation the data needs to go.","title":"Defining a connection"},{"location":"user_docs/get-requests/","text":"Get requests The api provides some simple get requests for basic information. Get status /status Returns \"api online\" if the api is running. Get datasets /datasets Returns datasets that can be used and called with a load operation.","title":"Basic requests"},{"location":"user_docs/get-requests/#get-requests","text":"The api provides some simple get requests for basic information.","title":"Get requests"},{"location":"user_docs/get-requests/#get-status","text":"/status Returns \"api online\" if the api is running.","title":"Get status"},{"location":"user_docs/get-requests/#get-datasets","text":"/datasets Returns datasets that can be used and called with a load operation.","title":"Get datasets"},{"location":"user_docs/operations_info/","text":"Operations What is an operation? An operation is a command. Something you want to be done. A load operation for example lets you load data from a database. An operation may take parameters that define the variable parts of it's job. The load operation for example takes a 'filename' parameter that tells it what datafile it needs to load. An operation may also take inputs. This is data it receives from other operations. The 'duplicate' operation for example must get the data it duplicates from a previous operation. An operation has ports, both for input as output. These allow some operations to take multiple inputs (like the merge operation) or give multiple outputs (like the duplicate operation). Defining an operation An operation is defined in JSON as follows: {\"id\":ID,\"operation\":operation_name,\"parameters\":{\"parameter_name_1\":parameter_1, \"parameter_name_2\":parameter_2, ...}} Example: {\"id\":2,\"operation\":\"load\",\"parameters\":{\"filename\":\"titanic\"}} It is a dictionary that defines an id, an operation and a dictionary of parameters id The id is an integer of your choice. Its outputs will be defined by this id in the response so you can know which operation returned what. It is also used to define the connections. operation This is a string with the name of the operation you want to be executed. parameters This is a dictionary in which you define the parameters for the operation. Each operation has its own set of parmeters it takes.","title":"operations"},{"location":"user_docs/operations_info/#operations","text":"","title":"Operations"},{"location":"user_docs/operations_info/#what-is-an-operation","text":"An operation is a command. Something you want to be done. A load operation for example lets you load data from a database. An operation may take parameters that define the variable parts of it's job. The load operation for example takes a 'filename' parameter that tells it what datafile it needs to load. An operation may also take inputs. This is data it receives from other operations. The 'duplicate' operation for example must get the data it duplicates from a previous operation. An operation has ports, both for input as output. These allow some operations to take multiple inputs (like the merge operation) or give multiple outputs (like the duplicate operation).","title":"What is an operation?"},{"location":"user_docs/operations_info/#defining-an-operation","text":"An operation is defined in JSON as follows: {\"id\":ID,\"operation\":operation_name,\"parameters\":{\"parameter_name_1\":parameter_1, \"parameter_name_2\":parameter_2, ...}} Example: {\"id\":2,\"operation\":\"load\",\"parameters\":{\"filename\":\"titanic\"}} It is a dictionary that defines an id, an operation and a dictionary of parameters","title":"Defining an operation"},{"location":"user_docs/operations_info/#id","text":"The id is an integer of your choice. Its outputs will be defined by this id in the response so you can know which operation returned what. It is also used to define the connections.","title":"id"},{"location":"user_docs/operations_info/#operation","text":"This is a string with the name of the operation you want to be executed.","title":"operation"},{"location":"user_docs/operations_info/#parameters","text":"This is a dictionary in which you define the parameters for the operation. Each operation has its own set of parmeters it takes.","title":"parameters"},{"location":"user_docs/operations/aggregate_operation/","text":"Aggregating data Operation name aggregate Purpose Aggregating data as in SQL Parameters Parameter Description Datatype Required Default group_by Column to group by string yes / agg_column Column to aggregate on string yes / agg_method Method by which to aggregate string yes / agg_method The API can handle following aggregation methods agg_method Description count counts rows avg average min minimum max maximum sum Total sum of everything Inputs Port Description type 0 Dataframe to aggregate on dataframe Outputs Port Description type 0 Dataframe dataframe","title":"aggregate"},{"location":"user_docs/operations/aggregate_operation/#aggregating-data","text":"","title":"Aggregating data"},{"location":"user_docs/operations/aggregate_operation/#operation-name","text":"aggregate","title":"Operation name"},{"location":"user_docs/operations/aggregate_operation/#purpose","text":"Aggregating data as in SQL","title":"Purpose"},{"location":"user_docs/operations/aggregate_operation/#parameters","text":"Parameter Description Datatype Required Default group_by Column to group by string yes / agg_column Column to aggregate on string yes / agg_method Method by which to aggregate string yes /","title":"Parameters"},{"location":"user_docs/operations/aggregate_operation/#agg_method","text":"The API can handle following aggregation methods agg_method Description count counts rows avg average min minimum max maximum sum Total sum of everything","title":"agg_method"},{"location":"user_docs/operations/aggregate_operation/#inputs","text":"Port Description type 0 Dataframe to aggregate on dataframe","title":"Inputs"},{"location":"user_docs/operations/aggregate_operation/#outputs","text":"Port Description type 0 Dataframe dataframe","title":"Outputs"},{"location":"user_docs/operations/decision_tree/","text":"Training a decision tree operation name decision_tree Purpose The purpose of the decision_tree operation is to train a decision tree Example of a decision tree: Parameters Parameter Description Datatype Required Default target The target value the decision tree predicts on string yes / Inputs Port Description type 0 Dataframe with data to train on dataframe It will attempt to train on all columns in the inputted dataframe. Outputs Port Description type 0 Trained decision tree model (can not be given outside of api) model 1 SVG image of trained decision tree (base64 encoded) model Note that a model can not be returned in JSON.","title":"decision_tree"},{"location":"user_docs/operations/decision_tree/#training-a-decision-tree","text":"","title":"Training a decision tree"},{"location":"user_docs/operations/decision_tree/#operation-name","text":"decision_tree","title":"operation name"},{"location":"user_docs/operations/decision_tree/#purpose","text":"The purpose of the decision_tree operation is to train a decision tree Example of a decision tree:","title":"Purpose"},{"location":"user_docs/operations/decision_tree/#parameters","text":"Parameter Description Datatype Required Default target The target value the decision tree predicts on string yes /","title":"Parameters"},{"location":"user_docs/operations/decision_tree/#inputs","text":"Port Description type 0 Dataframe with data to train on dataframe It will attempt to train on all columns in the inputted dataframe.","title":"Inputs"},{"location":"user_docs/operations/decision_tree/#outputs","text":"Port Description type 0 Trained decision tree model (can not be given outside of api) model 1 SVG image of trained decision tree (base64 encoded) model Note that a model can not be returned in JSON.","title":"Outputs"},{"location":"user_docs/operations/duplicate_operation/","text":"Duplicating data operation name duplicate Purpose The purpose of the duplicate operation is to duplicate a dataframe. This will output the same dataframe on port 0 and port 1. This is quite useless and can be circumvented entirely by simply having two connections have their 'from' be the same operation and their 'from_port' be the same port. Choose your own way of doing it. Parameters Duplicate operation has no parameters. Inputs Port Description type 0 Dataframe to duplicate dataframe Outputs Port Description type 0 The inputted dataframe dataframe 1 The inputted dataframe dataframe","title":"duplicate"},{"location":"user_docs/operations/duplicate_operation/#duplicating-data","text":"","title":"Duplicating data"},{"location":"user_docs/operations/duplicate_operation/#operation-name","text":"duplicate","title":"operation name"},{"location":"user_docs/operations/duplicate_operation/#purpose","text":"The purpose of the duplicate operation is to duplicate a dataframe. This will output the same dataframe on port 0 and port 1. This is quite useless and can be circumvented entirely by simply having two connections have their 'from' be the same operation and their 'from_port' be the same port. Choose your own way of doing it.","title":"Purpose"},{"location":"user_docs/operations/duplicate_operation/#parameters","text":"Duplicate operation has no parameters.","title":"Parameters"},{"location":"user_docs/operations/duplicate_operation/#inputs","text":"Port Description type 0 Dataframe to duplicate dataframe","title":"Inputs"},{"location":"user_docs/operations/duplicate_operation/#outputs","text":"Port Description type 0 The inputted dataframe dataframe 1 The inputted dataframe dataframe","title":"Outputs"},{"location":"user_docs/operations/filter_rows_operation/","text":"Filtering rows Operation name filter_rows Purpose Filtering data based on a set of conditions. For example: removing every row where the column \"income\" has a value above 1800. Parameters Parameter Description Datatype Required Default conditions A list of conditions list yes / include Include (true) or exclude (false) rows that comply with the conditions bool no true set_operation Should rows comply with all conditions (intersect) or at least one (union) string no intersect Condition A condition is a dictionary with following parameters. Parameter Description Datatype Required Default column The column you want to filter the data on string yes / comparator The comparison you want to make string yes / compare_with the column or value you want to compare with string, int, float yes / compare_with_type Defines what compare_with is (column or value) string yes / comparator Possible values: '==' '!=' '<' '>' '<=' '>=' For example, \"comparator\":\">\" will result in a dataset with only the rows where value in 'column' > 'compare_with'. compare_with_type Can be either 'column' or 'value' Inputs Port Description type 0 Dataframe to filter on dataframe Outputs Port Description type 0 filtered dataframe dataframe","title":"filter_rows"},{"location":"user_docs/operations/filter_rows_operation/#filtering-rows","text":"","title":"Filtering rows"},{"location":"user_docs/operations/filter_rows_operation/#operation-name","text":"filter_rows","title":"Operation name"},{"location":"user_docs/operations/filter_rows_operation/#purpose","text":"Filtering data based on a set of conditions. For example: removing every row where the column \"income\" has a value above 1800.","title":"Purpose"},{"location":"user_docs/operations/filter_rows_operation/#parameters","text":"Parameter Description Datatype Required Default conditions A list of conditions list yes / include Include (true) or exclude (false) rows that comply with the conditions bool no true set_operation Should rows comply with all conditions (intersect) or at least one (union) string no intersect","title":"Parameters"},{"location":"user_docs/operations/filter_rows_operation/#condition","text":"A condition is a dictionary with following parameters. Parameter Description Datatype Required Default column The column you want to filter the data on string yes / comparator The comparison you want to make string yes / compare_with the column or value you want to compare with string, int, float yes / compare_with_type Defines what compare_with is (column or value) string yes /","title":"Condition"},{"location":"user_docs/operations/filter_rows_operation/#comparator","text":"Possible values: '==' '!=' '<' '>' '<=' '>=' For example, \"comparator\":\">\" will result in a dataset with only the rows where value in 'column' > 'compare_with'.","title":"comparator"},{"location":"user_docs/operations/filter_rows_operation/#compare_with_type","text":"Can be either 'column' or 'value'","title":"compare_with_type"},{"location":"user_docs/operations/filter_rows_operation/#inputs","text":"Port Description type 0 Dataframe to filter on dataframe","title":"Inputs"},{"location":"user_docs/operations/filter_rows_operation/#outputs","text":"Port Description type 0 filtered dataframe dataframe","title":"Outputs"},{"location":"user_docs/operations/load_operation/","text":"Loading data operation name load Purpose The purpose of the load operation is to load in data. Parameters Parameter Description Datatype Required Default filename The datafile to load string yes / sample sample size bool or int no False randomize_sample If true, randomizes what data is used for the sample bool no False Inputs The load operation does not take any inputs. Outputs Port Description type 0 Loaded dataframe dataframe","title":"load"},{"location":"user_docs/operations/load_operation/#loading-data","text":"","title":"Loading data"},{"location":"user_docs/operations/load_operation/#operation-name","text":"load","title":"operation name"},{"location":"user_docs/operations/load_operation/#purpose","text":"The purpose of the load operation is to load in data.","title":"Purpose"},{"location":"user_docs/operations/load_operation/#parameters","text":"Parameter Description Datatype Required Default filename The datafile to load string yes / sample sample size bool or int no False randomize_sample If true, randomizes what data is used for the sample bool no False","title":"Parameters"},{"location":"user_docs/operations/load_operation/#inputs","text":"The load operation does not take any inputs.","title":"Inputs"},{"location":"user_docs/operations/load_operation/#outputs","text":"Port Description type 0 Loaded dataframe dataframe","title":"Outputs"},{"location":"user_docs/operations/merge_operation/","text":"Merging data operation name merge Purpose The purpose of the merge operation is to merge two dataframes. This will perform a join operation as in SQL. Parameters Parameter Description Datatype Required Default column_1 Column from inputted 'left' dataframe to join with string yes / column_2 Column from inputted 'right' dataframe to join with string yes / join_type Type of join to perform string no inner Join types join_type can be one of four values. inner outer left right These work the same as SQL joins Inputs Port Description type 0 'Left' dataframe to join on dataframe 1 'Right' dataframe to join on dataframe Outputs Port Description type 0 Merged dataframe dataframe","title":"merge"},{"location":"user_docs/operations/merge_operation/#merging-data","text":"","title":"Merging data"},{"location":"user_docs/operations/merge_operation/#operation-name","text":"merge","title":"operation name"},{"location":"user_docs/operations/merge_operation/#purpose","text":"The purpose of the merge operation is to merge two dataframes. This will perform a join operation as in SQL.","title":"Purpose"},{"location":"user_docs/operations/merge_operation/#parameters","text":"Parameter Description Datatype Required Default column_1 Column from inputted 'left' dataframe to join with string yes / column_2 Column from inputted 'right' dataframe to join with string yes / join_type Type of join to perform string no inner","title":"Parameters"},{"location":"user_docs/operations/merge_operation/#join-types","text":"join_type can be one of four values. inner outer left right These work the same as SQL joins","title":"Join types"},{"location":"user_docs/operations/merge_operation/#inputs","text":"Port Description type 0 'Left' dataframe to join on dataframe 1 'Right' dataframe to join on dataframe","title":"Inputs"},{"location":"user_docs/operations/merge_operation/#outputs","text":"Port Description type 0 Merged dataframe dataframe","title":"Outputs"},{"location":"user_docs/operations/predict_operation/","text":"Predicting operation name predict Purpose The purpose of the predict operation is to use an inputted decision tree to make a prediction. Parameters Parameter Description Datatype Required Default values list with lists of values to make predictions on list of lists no / Inputs Port Description type 0 decision tree model model 1 dataframe to predict on dataframe port 1 is not required. Outputs Port Description type 0 inputted dataframe with added 'predictions' column with predictions for each row dataframe 1 list of outcomes predicted from values parameter list","title":"predict"},{"location":"user_docs/operations/predict_operation/#predicting","text":"","title":"Predicting"},{"location":"user_docs/operations/predict_operation/#operation-name","text":"predict","title":"operation name"},{"location":"user_docs/operations/predict_operation/#purpose","text":"The purpose of the predict operation is to use an inputted decision tree to make a prediction.","title":"Purpose"},{"location":"user_docs/operations/predict_operation/#parameters","text":"Parameter Description Datatype Required Default values list with lists of values to make predictions on list of lists no /","title":"Parameters"},{"location":"user_docs/operations/predict_operation/#inputs","text":"Port Description type 0 decision tree model model 1 dataframe to predict on dataframe port 1 is not required.","title":"Inputs"},{"location":"user_docs/operations/predict_operation/#outputs","text":"Port Description type 0 inputted dataframe with added 'predictions' column with predictions for each row dataframe 1 list of outcomes predicted from values parameter list","title":"Outputs"},{"location":"user_docs/operations/preprocess_operation/","text":"Preprocessing operation name preprocess Purpose The purpose of the preprocess operation is to ensure data is usable for training models by handling missing information and encodig non-numeric variables. Parameters Parameter Description Datatype Required Default actions dictionary with the column name as key and for value a preprocess or list of preprocesses. dictionary yes / Preprocesses fillna fills in missing value. drop drops rows with missing value. encode encodes categorical variables to numerical variables. If you do both fillna and drop on a column it will only do the fillna. Inputs Port Description type 0 dataframe to preprocess dataframe Outputs Port Description type 0 preprocessed dataframe dataframe","title":"preprocess"},{"location":"user_docs/operations/preprocess_operation/#preprocessing","text":"","title":"Preprocessing"},{"location":"user_docs/operations/preprocess_operation/#operation-name","text":"preprocess","title":"operation name"},{"location":"user_docs/operations/preprocess_operation/#purpose","text":"The purpose of the preprocess operation is to ensure data is usable for training models by handling missing information and encodig non-numeric variables.","title":"Purpose"},{"location":"user_docs/operations/preprocess_operation/#parameters","text":"Parameter Description Datatype Required Default actions dictionary with the column name as key and for value a preprocess or list of preprocesses. dictionary yes /","title":"Parameters"},{"location":"user_docs/operations/preprocess_operation/#preprocesses","text":"fillna fills in missing value. drop drops rows with missing value. encode encodes categorical variables to numerical variables. If you do both fillna and drop on a column it will only do the fillna.","title":"Preprocesses"},{"location":"user_docs/operations/preprocess_operation/#inputs","text":"Port Description type 0 dataframe to preprocess dataframe","title":"Inputs"},{"location":"user_docs/operations/preprocess_operation/#outputs","text":"Port Description type 0 preprocessed dataframe dataframe","title":"Outputs"},{"location":"user_docs/operations/select_columns_operation/","text":"Selecting columns Operation name select_columns Purpose The purpose of the select_columns operation is to select columns from a dataframe and continue with a new dataframe with only these columns. Parameters Parameter Description Datatype Required Default columns The columns to be selected list of strings yes / include Include (true) or exclude (false) columns bool no true Inputs Port Description type 0 Dataframe to select columns from dataframe Outputs Port Description type 0 Inputted dataframe with only the columns to select on dataframe Port 1 effectively removes the columns.","title":"select_columns"},{"location":"user_docs/operations/select_columns_operation/#selecting-columns","text":"","title":"Selecting columns"},{"location":"user_docs/operations/select_columns_operation/#operation-name","text":"select_columns","title":"Operation name"},{"location":"user_docs/operations/select_columns_operation/#purpose","text":"The purpose of the select_columns operation is to select columns from a dataframe and continue with a new dataframe with only these columns.","title":"Purpose"},{"location":"user_docs/operations/select_columns_operation/#parameters","text":"Parameter Description Datatype Required Default columns The columns to be selected list of strings yes / include Include (true) or exclude (false) columns bool no true","title":"Parameters"},{"location":"user_docs/operations/select_columns_operation/#inputs","text":"Port Description type 0 Dataframe to select columns from dataframe","title":"Inputs"},{"location":"user_docs/operations/select_columns_operation/#outputs","text":"Port Description type 0 Inputted dataframe with only the columns to select on dataframe Port 1 effectively removes the columns.","title":"Outputs"}]}